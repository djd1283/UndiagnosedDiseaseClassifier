diff --git a/datasets.py b/datasets.py
index 7d2e926..f2deba4 100644
--- a/datasets.py
+++ b/datasets.py
@@ -57,10 +57,11 @@ class UndiagnosedFeatureExtractor:
     def __init__(self):
         self.tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')
         self.gpt = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt').cuda()
-        self.embedder = SentenceTransformer('bert-base-nli-mean-tokens')
+        self.embedder = SentenceTransformer('bert-base-nli-mean-tokens').cuda()
         self.pos_phrase = "I have an undiagnosed disease. "
         self.neg_phrase = ""
         # self.phrase_gpt_score = gpt_log_prob_score([self.phrase], self.gpt, self.tokenizer)
+        self.pos_phrase_emb = self.embedder.encode([self.pos_phrase])[0]
 
     def extract_features(self, texts):
         # TODO fix problem that GPT loss values are normalized for sequence length
@@ -68,20 +69,24 @@ class UndiagnosedFeatureExtractor:
         pos_phrase_and_texts = [text + self.pos_phrase for text in texts]
         # neg_phrase_and_texts = [self.neg_phrase + text for text in texts]
 
+        sbert_scores = sentence_bert_score(texts, [self.pos_phrase] * len(texts), self.embedder, return_all=True)
+
         pos_phrase_text_gpt_scores = gpt_log_prob_score(pos_phrase_and_texts, self.gpt, self.tokenizer, return_all=True)
         # neg_phrase_text_gpt_scores = gpt_log_prob_score(neg_phrase_and_texts, self.gpt, self.tokenizer, return_all=True)
 
         phrase_text_mmis = []
-        for pos_phrase_score, text_score in zip(pos_phrase_text_gpt_scores, text_gpt_scores):
+        for pos_phrase_score, text_score, text_emb in zip(pos_phrase_text_gpt_scores, text_gpt_scores):
             # negate loss for log probability
             phrase_text_mmi = (pos_phrase_score - text_score) / text_score
             phrase_text_mmis.append(phrase_text_mmi)
+
         text_lens = [math.log(len(text.split())) for text in texts]
 
-        return np.array(list(zip(text_gpt_scores, phrase_text_mmis, text_lens)))
+        return np.array(list(zip(sbert_scores, text_gpt_scores, phrase_text_mmis, text_lens)))
 
 
-def sentence_bert_score(r1, r2, embedder):
+def sentence_bert_score(r1, r2, embedder, return_all=False):
+    """Compute cosine similarity between SBERT embeddings of corresponding sentences in r1 and r2."""
     with torch.no_grad():
         r1_embs = embedder.encode(r1)
         r2_embs = embedder.encode(r2)
@@ -91,7 +96,10 @@ def sentence_bert_score(r1, r2, embedder):
             bert_score = np.dot(r1_emb, r2_emb) / np.linalg.norm(r1_emb) / np.linalg.norm(r2_emb)
             bert_scores.append(bert_score)
 
-    return np.mean(bert_scores)
+    if return_all:
+        return bert_scores
+    else:
+        return np.mean(bert_scores)
 
 
 def gpt_log_prob_score(sentences, model, tokenizer, max_len=256, return_all=False):
diff --git a/train_undiagnosed_classifier.py b/train_undiagnosed_classifier.py
index 4bde62d..e4c37f2 100644
--- a/train_undiagnosed_classifier.py
+++ b/train_undiagnosed_classifier.py
@@ -11,7 +11,7 @@ import numpy as np
 accepted_submissions_file = 'data/accepted_submissions.tsv'
 negative_submissions_file = 'data/negative_submissions.tsv'
 rejected_submissions_file = 'data/rejected_submissions.tsv'
-n_features = 3
+n_features = 4
 n_epochs = 1000
 learning_rate = 0.001
 batch_size = 2
